# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KfQq3TFFUgOesSjoPRk7PbgLWwJ-dLK1

# Twitter Sentiment Analysis
"""

#Connect Google drive to colab
from google.colab import drive
drive.mount('/gdrive')

"""### About Dataset
Source : Kaggle

Context
The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.

Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, our objective is to predict the labels on the test dataset.

Content
Full tweet texts are provided with their labels for training data.
Mentioned users' username is replaced with @user.
"""



"""### Main implemenatations 
1.Dataset source : Kagge ,
2.Problem statement : Sentiment analysis of a tweet review
3.preprocessing:Tokenization,padding
4.Batch normilization, dropouts,
5.adam activation and metrics used is accuarcy
6.Early stopping
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

import tensorflow as tf

"""### Loading dataset"""

df = pd.read_csv('/gdrive/My Drive/Colab Notebooks/RNN/RNN Variants/TheCodeWork/Twitter Sentiment Analysis/train.csv',header=0)
df_validation = pd.read_csv('/gdrive/My Drive/Colab Notebooks/RNN/RNN Variants/TheCodeWork/Twitter Sentiment Analysis/test.csv',header=0)

df_validation_original = df_validation.copy()

df.shape

df.head()

df.isnull().sum() # Checking missing value if any

"""No missing value observed."""

df.loc[0, 'tweet']

y = df['label']
X = df.drop('label',axis =1)

"""### Checking data imbalance"""

round((len(y[y == 0])/len(y))*100,2) # % of label 0

round((len(y[y == 1])/len(y))*100,2) # % of label 1

"""Looking at % of both labels , data is highly imbalanced.Hence , We will do data augumenation to make the data balanced.

## 1.Class balancing with RandomOverSampler
"""

from imblearn.over_sampling import RandomOverSampler
# define oversampling strategy
oversample = RandomOverSampler(sampling_strategy='minority')


# fit and apply the transform
X_over, y_over = oversample.fit_resample(X, y)

"""Checking data imbalance post data balancing"""

round((len(y_over[y_over == 0])/len(y_over))*100,2) # % of label 0

round((len(y_over[y_over == 1])/len(y_over))*100,2) # % of label 1

"""Now, data is balanced.

### Splitting into train-test split
"""

X_train, X_test, y_train, y_test = train_test_split(X_over,y_over,test_size = 0.3,random_state = 42)

X_train.shape

X_test.shape

"""### Build the Tokenizer"""

type(X_train)

X_train[0:1] # Pputting all X_train data into list and taking 1st

top_words = 10000 #Vocablury size
t = tf.keras.preprocessing.text.Tokenizer(num_words = top_words) # num_words -> Vocablury size

X_train = X_train['tweet'].squeeze() # converting into Dataframe into series

type(X_train)

#Fit tokenizer with actual training data.Tokelization means identifying all indvisual unique words and creating a dictionary out of it.
t.fit_on_texts(X_train.tolist())

type(t)

#Vocabulary.Here index is assigned against each word of entire tweet
t.word_index

"""# Prepare Training and Test Data"""

X_train[0:1] # view first data after tokenization

X_train = t.texts_to_sequences(X_train.tolist())

X_train[0:1][0] # checking first train tweet after tokenization

X_test = X_test['tweet'].squeeze() # converting into Dataframe into series

X_test = t.texts_to_sequences(X_test.tolist())  # coverting text to sequence of numbers

"""# Pad Sequences"""

#Define maximum number of words to consider in each review
max_review_length = 40

#Pad training and test reviews
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,maxlen = max_review_length,padding = 'pre')
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,maxlen = max_review_length,padding = 'pre')

len(X_train[0:1][0]) # printing lenght of first padded train tweet

X_train[0:1]

X_train.shape

X_test.shape

"""# Model building"""

#Initialize model
tf.keras.backend.clear_session()
model = tf.keras.Sequential()

model.add(tf.keras.layers.Embedding(top_words +1,60,input_length=max_review_length))  #Vocablury size = top_words,#Embedding size = 60,every word lengh is of 60 numbers. max_review_length = Number of words in each tweet

model.output

# Add LSTM layer with RNN state size of 256
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.50))

#model.add(tf.keras.layers.LSTM(256,dropout = 0.25))
model.add(tf.keras.layers.LSTM(256,dropout = 0.25)) #RNN State - size of cell state and hidden state is 256

model.output

#Adding Dense layer for output
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

#model compilation
model.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])

model.summary()

#Executing graph
from tensorflow.keras.callbacks import EarlyStopping # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.
early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'loss',patience =5,mode = 'auto')

#model fit
history = model.fit(X_train,y_train,epochs =50,batch_size=64,validation_data=(X_test,y_test),callbacks = [early_stopping])

"""### Observation
We observe that training stops at 38 epoch as losses started increasing post that.
Training accuracy : 99.85
Test accuracy : 98.99
Test Loss : 0.0583
"""

print(history.history.keys())

#Plotting model loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""### Validation loss is almost trending same post ~ 30 epoch."""

### Validation of model on separate given test data
type(df_validation)

df_validation = pd.read_csv('/gdrive/My Drive/Colab Notebooks/RNN/RNN Variants/TheCodeWork/Twitter Sentiment Analysis/test.csv',header=0)

df_validation = df_validation['tweet'].squeeze() # converting into Dataframe into series

#Fit tokenizer with actual training data.Tokelization means identifying all indvisual unique words and creating a dictionary out of it.
t.fit_on_texts(df_validation.tolist())

type(t)

#Vocabulary.Here index is assigned against each word of entire tweet
t.word_index

df_validation = t.texts_to_sequences(df_validation.tolist())

df_validation[0:1][0] # checking first validation tweet after tokenization

"""Pad Sequences"""

df_validation = tf.keras.preprocessing.sequence.pad_sequences(df_validation,maxlen = max_review_length,padding = 'pre')

len(df_validation[0:1][0]) # printing lenght of first padded validation tweet

df_validation[0:1] # Checking tensor of first validation data

#Get prediction for validation data

validationPredict = model.predict(df_validation)

type(validationPredict)

validationPredict = pd.DataFrame(validationPredict)

# Appending y_test_df and y_pred_1
output = pd.concat([df_validation_original, validationPredict],axis=1)

type(output)

output.head()

output.columns

output.rename(columns={'id': 'id', 'tweet': 'tweet',0:'Predicted_prob'}, inplace=True) #Renaming columns

output.head()

output['final_predicted'] = output['Predicted_prob'].map(lambda x: 1 if x > 0.5 else 0)

output.head()

df_validation = pd.read_csv('/gdrive/My Drive/Colab Notebooks/RNN/RNN Variants/TheCodeWork/Twitter Sentiment Analysis/test.csv',header=0)

output = output.to_excel('/gdrive/My Drive/Colab Notebooks/RNN/RNN Variants/TheCodeWork/Twitter Sentiment Analysis/output.xlsx')